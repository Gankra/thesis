\chapter{Designing APIs for Trust}
\label{ch:design}

It has been our experience that almost everything you want to express at a high level
can be safely, efficiently, and usably expressed in an ownership-oriented system.
However this does \emph{not} mean that you can express it however you please!
In order to be safe, efficient, and usable the API itself must be designed in
terms of data trust. If the API isn't designed with trust and ownership in mind,
these goals will likely be compromised.

We have already seen how ownership eliminates use-afters, view invalidation,
and data races. However we have not yet seen how indexing is addressed.




\section{Indexing}

Arrays are overwhelmingly
the most common and important data structure in all of programming. Basically every
program will do some array processing -- even \emph{hello world} is just copying an
array of bytes to stdout. Array processing is so pervasive that Rust provides no
less than \emph{four} different ways to index into an array, each corresponding to
one of the trust strategies: naive, paranoid, implicitly suspicious, and
explicitly suspicious.

First and foremost, Rust provides the two most popular interfaces: completely
unchecked indexing (via Unsafe Rust), and implicitly checked indexing (which
unwinds the program if it fails). Nothing particularly surprising or
novel there.

Slightly more interesting is how Rust provides an explicitly checked option.
Explicit checking is most commonly done through the Option and Result types
in Rust. If an operation can fail, it will return one of these types.
These types are tagged unions, which means they can contain different
types at runtime. `Option<T>` can be `Some(T)` or `None`, while `Result<T, E>`
can be `Ok(T)` or `Err(E)`. By owning the data they wrap, these types can control
how it's accessed. Both of them provide a multitude of ways to access their
contents in an explicitly checked way.

Explicitly checked indexing returns an Option, so if we want to explicitly
handle the failure condition, we can do any of the following:

\begin{minted}{rust}
// Exhaustively match on the possible choices
match array.get(index) {
    Some(elem) => println!("Elem: {}", elem),
    None       => println!("Nothing"),
}

// Only handle one pattern
if let Some(elem) = array.get(index) {
    println!("Elem: {}", elem);
}

// Execute a callback if the value is Some
array.get(index).map(|elem| {
    println!("Elem: {}", elem);
});

// ... and more
\end{minted}

It's worth noting that checked indexing is surprisingly performant here.
First off, these bounds checks are by definition trivially predictable in a
correct program. So the overhead at the hardware level is quite small. That said,
having to do the checks at all is the worst-case. A good optimizing compiler
(like LLVM) can optimize away bounds checks in many ``obviously correct'' cases.
For instance, the following code doesn't actually perform any bounds checks
when optimized, because LLVM can see that the way the indices are generated
trivially satisfies the bounds checking.

\begin{minted}{rust}
let mut x = 0;
for i in 0 .. arr.len() {
    // implicit checked indexing
    x += arr[i];
}
\end{minted}

Indeed, if you can convince LLVM to not completely inline and constant-fold
this code away, it will even successfully vectorize the loop!

However compiler optimizations are brittle things that can break under even
seemingly trivial transformations. For instance, changing this code to simply
iterate over the array \emph{backwards} completely broke LLVM's analysis and produced
naive code that adds the integers one at a time with bounds checking. This is
perhaps the most serious cost of bounds checks: inhibiting other optimizations.

If we really care about avoiding this cost, we can't just rely on the optimizer
to magically figure out what we're doing, we need to actually not do bounds
checking. We can use the unsafe unchecked indexing, but we'd rather not resort
to unsafe code unless totally necessary. What we really want here is Rust's final
solution to indexing: the paranoid one.

This code is hard to optimize safely
because we've pushed too much of the problem at hand to the user of the array.
They need to figure out how to generate the access pattern, and we in turn can't
trust it. If the \emph{array} handles generating the access pattern \emph{and}
acquiring the elements, then all the bounds checks can be eliminated at the
source level in a way that's safe to the end-user. This is handled by a tried
and true approach: iterators.

\begin{minted}{rust}
let mut sum = 0;
for x in arr.iter() {
    sum += *x;
}
\end{minted}

This produces the same optimized code as the original indexing-based solution,
but more importantly, it's more robust to transformations. Iterating
backwards now also produces vectorized unchecked code, because the optimizer
has less to prove about the program's behaviour. As an added bonus, client code
ends up simplified as well, as all the error-prone iteration boilerplate has
been eliminated.

Of course, this doesn't come for free. Iterators are effectively special-casing
certain access patterns at the burden of the interface's implementor. In the
case of iterators, this burden is completely justified. Linear iteration
is incredibly common, and therefore well worth the specialization. But if the
user wants to binary search an array without bounds checks, iterators do them no
good.

This is why Rust also provides the raw unchecked indexing API. If users have a
special case and really need to optimize it to death, they can drop down to
the raw API and regain full control. In the case of arrays, the raw API is
trivial to provide and obvious, so there's little worry of the maintenance burden
or unnecessarily exposing implementation details. Unfortunately, this isn't true for all
interfaces (what's the raw API for searching an ordered map?).






\section{External and Internal Interfaces}

It is relatively common to use maps as \emph{accumulators}. The most trivial example
of this is using a map to count the number of occurrences of each key. Accumulators
are interesting because special logic must usually be performed when a key is
seen for the first time. In the case of counting keys, the first time we see a key
we want to insert the value 1, but each subsequent time we see that key
we want to instead increment the count.

Naively, one may write this as follows:

\begin{minted}{rust}
if map.contains(&key) {
    map[key] += 1;
} else {
    map.insert(key, 1);
}
\end{minted}

Those concerned with performance may see an obvious problem with this implementation:
we're unnecessarily looking up each key twice. Instead we would like to search in the
map only once, and execute different logic depending on if the key was found or not
\emph{without} performing the search again.

Before Rust 1.0 was released, there existed a family of functions for doing exactly
that. For the simple case of a counter, we only need to provide a default value,
which works well enough:

\begin{minted}{rust}
*map.find_or_insert(key, 0) += 1;
\end{minted}

However the default value might be expensive to create for some kinds of accumulators.
As such, we'd like to avoid constructing it unless we know it's required.
And so, \mintinline{rust}{find_or_insert_with} was provided, which took a
\emph{function} that computed the default value:

\begin{minted}{rust}
*map.find_or_insert_with(key, expensive_default_func) += 1;
\end{minted}

However this interface had the problem that it could be difficult to tell which
case was found. This is where things started to fall apart. In order to support
this, a function that took \emph{two} functions was created; one for each case. However this
design was problematic because each function
may want to capture the same affine data by-value. We know this is sound because only
one of the two functions will be called, but the compiler doesn't understand that.
So an extra argument was added which would be passed to the function that \emph{was}
called.

\begin{minted}{rust}
map.update_with_or_insert_with(key, capture,
                               compute_default_func,
                               update_existing_func);
\end{minted}

While the first APIs seemed quite reasonable, this final API was nightmarish. Worse,
it didn't even accommodate all the use cases people came up with. Some wanted
to \emph{remove} the key from the map under some conditions, which would necessitate
a whole new family of update functions. The problem is that this design is what the
Rust community calls an \emph{internal} interface. Internal interfaces require the client
to execute \emph{inside} the interface's own code by passing functions that should be
called at the appropriate time. In some cases, internal interfaces can be convenient
or even more efficient, but they generally suffer from how little control they give
the client.

We solved this inflexibility by instead providing an \emph{external} interface.
Internal interfaces execute the entire algorithm at once, invoking the client to
handle important cases. An external interface instead requires the client to drive the
execution of the algorithm manually. At each step, the algorithm returns some
value that summarizes the current state, and exposes relevant operations.

For the accumulator problem, we created the \emph{entry} API. The basic idea of the
entry API is simple: execute the insertion algorithm up until we determine
whether the key already existed. Once this is known, take a \emph{snapshot} of
the algorithm state, and store it in a tagged union with two states: Vacant or
Occupied. The consumer of the interface must then match on the union to determine
which state the algorithm is in. The Vacant state exposes only one operation,
insert, as this is the only valid operation to perform on an empty entry.
The Occupied state, on the other hand, exposes the ability to overwrite, remove,
or take a pointer to the old value.

In its most general form, usage looks as follows:

\begin{minted}{rust}
// Search for this key, and capture whether it's in the map or not
match map.entry(key) {
    Vacant(e) => {
        // The key is not in the map, compute the new value
        let value = expensive_default(capture);
        e.insert(value);
    }
    Occupied(e) => {
        // The key is in the map, update the value
        expensive_update(e.get_mut(), capture);

        // Conditionally remove the key from the map altogether
        if *e.get() == 0 {
            e.remove();
        }
    }
}
\end{minted}

Control flow is now driven by the client of the API, and not the API itself.
No additional interfaces need to be added to accommodate all the different
actions that are desired, and no additional lookups are performed.

Of course, this is a significant ergonomic regression for simple cases like
counting, for which convenience methods were added to the Entry result:

\begin{minted}{rust}
*map.entry(key).or_insert(0) += 1;
\end{minted}

One may question if we have then gained much if we're still adding some of
the old interfaces this design was intended to replace, but there is an important
difference. Before, we were required to add new interfaces to accommodate increasingly
complex use cases. Now, we are adding new interfaces to accommodate increasingly
\emph{common} use cases. We believe this to be the more correct way for an interface to
grow; adding conveniences for idioms, rather than adding nightmares for special cases.

One important question about this interface is whether it's \emph{sound}. After all,
we're taking a snapshot of the internal state of a collection, and then yielding
control to the client explicitly to mutate the collection. Indeed, in many languages
this interface would be dangerous without runtime checks for exactly the same
reason that iterators are dangerous. And for exactly the same reason that iterators
can safely be used without any runtime checks, so can entries: ownership!

An Entry mutably borrows the map it points into, which means that while it
exists, we know that it has exclusive access to the map. It can trust that the
algorithmic state it recorded will never become inconsistent. Further, any operation
that the entry exposes that would invalidate itself (such as inserting into
a vacant entry or removing from an occupied entry) consumes it by-value,
preventing further use.

The comparison to iterators is particularly apt here because iterators
are yet another external interface. They require the consumer to repeatedly
request the next element, returning an Option that is None if the iteration
is complete. Like entries, iterators in Rust were once provided as an internal
interface. Iteration required a function to be passed to the iterator, which
it would then execute on every element.

The most fundamental weakness of this design is that it was impossible to
concurrently iterate over two sources at once. Each iterator could only
be executed to completion at once. With external iterators, concurrent iteration
is simply alternating which iterator to ask for the next element.




\section{Hacking Generativity onto Rust}

Given the array iteration example, one might wonder if it's sufficient for the
array to simply provide the indices, and not immediately convert them into elements.
This would be a more composable API with a reduced implementation burden.

Perhaps this API could be used something like this:

\begin{minted}{rust}
let mut sum = 0;
for i in arr.indices() {
    sum += arr[i];
}
\end{minted}

Unfortunately, this doesn't immediately get us anywhere. This is no different
than the original example which produced its own iteration sequence. As soon
as the array loses control of the yielded indices, they are \emph{tainted} and all
trust is lost. After all, they're just integers, and integers can come from anywhere.
One may consider wrapping the integers in a new type that doesn't
expose the values to anyone but the array, preventing them from being
tampered with, but this is still insufficient unless the \emph{origin} of
these values can be verified. Given two arrays, it mustn't be possible to index
one using the indices of the other:

\begin{minted}{rust}
let mut sum = 0;
for i in arr1.indices() {
    // index arr2 using arr1's trusted indices
    sum += arr2[i];
}
\end{minted}

This is a problem many static type systems generally struggle to model, because
they don't provide tools to talk about the origin or destination of a particular
instance of a type. In addition, even if we could prevent this, we would have to
deal with the problem of temporal validity. The indices into an array are only
valid as long as the array's length doesn't change:

\begin{minted}{rust}
// get a trusted index into the array
let i = arr.indices().next().unwrap();
// shrink the array
arr.pop();
// index the array with an outdated index
let x = arr[i];
\end{minted}

By pure accident, Rust provides enough tools to solve
this problem. It turns out that lifetimes in conjunction with some other features
are sufficient to introduce \emph{generativity} into the type system. Generativity is
a limited system that can solve some of the problems usually reserved for
dependent typing. I won't lie: this is an ugly hack, and I don't expect it
to see much use, although early drafts of this work have inspired the creation
of at least one library \cite{bluss-indexing}. Regardless, it demonstrates the power of
the ownership system.

In order to encode sound unchecked indexing, we need a way for types to talk about
particular instances of other types. In this case, we specifically need a way
to talk about the relationship between a particular array, and the indices it has
produced. Rust's lifetime system, it turns out, gives us exactly that. Every
instance of a value that contains a lifetime (e.g. a pointer) is referring to
some particular region of code. Further, code can require that two lifetimes
satisfy a particular relationship. Typically all that is required is that
one outlives the other, but it is possible to require strict equality.

The basic idea is then as follows: associate an array and its indices with
a particular region of code. Unchecked indexing then simply requires that the
input array and index are associated with the same region.
However care must be taken, as Rust was not designed to support this. In
particular, the borrow checker is a constraint solver that will attempt do
everything in its power to make code type-check. As such, if it sees
a constraint that two lifetimes must be equal, it may expand their scopes in
order to satisfy this constraint. Since we're trying to explicitly use equality
constraints to prevent certain programs from compiling, this puts us at odds
with the compiler.

In order to accomplish our goal, we need to construct a black box that the
borrow checker can't penetrate. This involves two steps: disabling lifetime variance,
and creating an inaccessible scope. Disabling variance is relatively straight
forward. Several generic types disable variance in order to keep Rust sound.
Getting into the details of this is fairly involved, so we will just
say we can wrap a pointer in the standard library's Cell type as follows:

\begin{minted}{rust}
struct Index<'id> {
    data: usize,
    _id: Cell<&'id u8>,
}
\end{minted}

The $'id$ syntax is new here. Although most Rust programs can avoid declaring
lifetimes, more advanced usage necessitates declaring them as generic arguments.
In this case, we're externally declaring that the Index type contains something with
a lifetime called $id$. Internally we're declaring this to be the lifetime of some
pointer to a byte.

Of course we don't \emph{actually} want to store a pointer at runtime, because we're
only interested in creating a lifetime for the compiler to work with. Needing to
signal that we contain a lifetime or type that we don't directly store is a
sufficiently common requirement in Unsafe Rust that the language provides a primitive
for exactly this: PhantomData. PhantomData tells the type system "pretend I
contain this", while not actually taking up any space at runtime.

\begin{minted}{rust}
// Synonym to avoid writing this out a lot
type Id<'id> = PhantomData<Cell<&'id u8>>;

struct Index<'id> {
    data: usize,
    _id: Id<'id>,
}
\end{minted}

Now Rust believes it's unsound to freely resize the $id$ lifetime. However, as
written, there's nothing that specifies where this $id$ should come from. If
we're not careful, Rust could \emph{still} unify lifetimes incorrectly if it notices
there's no actual constraints on them. Consider the following kind of program
we're trying to prevent:

\begin{minted}{rust}
let arr1: Array<'a> = ...;
let arr2: Array<'b> = ...;
let index: Index<'a> = arr1.get_index();

// This should fail to compile;
// trying to index Array<'b> with Index<'a>
let x = arr2[index];
\end{minted}

If we don't constrain the lifetimes $a$ and $b$, then the constraint solver
will see only the following system:

\begin{itemize}
 \item $a$ and $b$ are invariant lifetimes
 \item indexing requires $a = b$
\end{itemize}

Which has the obvious solution of $a = b = anything$. We need to apply some
constraint to $a$ and $b$ to prevent Rust from unifying them. Within a single
function the compiler has perfect information and can't be tricked. However
Rust explicitly does not perform inter-procedural analysis, so we can apply
constraints with functions. In particular, Rust has to assume that the \emph{input}
to a function is a fresh lifetime, and can only unify them if that function
provides constraints:

\begin{minted}{rust}
fn foo<'a, 'b>(x: &'a u8, y: &'b u8) {
    // cannot assume x has any relationship to y, since they
    // have their own lifetimes
}

fn bar<'a>(x: &'a u8, y: &'a u8) {
    // x has the same lifetime as y, since they share 'a
}
\end{minted}

Therefore, for every fresh lifetime we wish to construct, we require a new
function call. We can do this as ergonomically as possible (considering this
is a hack) by using closures:

\begin{minted}{rust}
fn main() {
    let arr1 = &[1, 2, 3, 4, 5];
    let arr2 = &[10, 20, 30];

    // Yuck! So much nesting!
    make_id(arr1, move |arr1| {
    make_id(arr2, move |arr2| {
        // Within this closure, Rust is forced to assume
        // that the lifetimes associated with arr1 and
        // arr2 originate in their respective make_id
        // calls. As such, it is unable to unify them.

        // Iterate over arr1
        for i in arr1.indices() {
            // Will compile, no bounds checks
            println!("{}", arr1.get(i));

            // Won't compile
            println!("{}", arr2.get(i));
        }
    });
    });
}

// An Invariant Lifetime
type Id<'id> = PhantomData<Cell<&'id u8>>;

// A wrapper around an array that has a unique lifetime
struct Array<'arr, 'id> {
    array: &'arr [i32],
    _id: Id<'id>,
}

// A trusted in-bounds index to an Array with the same lifetime
struct Index<'id> {
    idx: usize,
    _id: Id<'id>,
}

// A trusted iterator of in-bounds indices into an Array
// with the same lifetime
struct Indices<'id> {
    min: usize,
    max: usize,
    _id: Id<'id>,
}

// Given a normal array, wrap it to have a unique lifetime
// and pass it to the given function
pub fn make_id<'arr, F>(array: &'arr [i32], func: F)
    where F: for<'id> FnOnce(Array<'arr, 'id>),
{
    let arr = Array { array: array, _id: PhantomData };
    func(arr);
}

impl<'arr, 'id> Array<'arr, 'id> {
    // Access the following index without bounds checking
    pub fn get(&self, idx: Index<'id>) -> &'arr i32 {
        unsafe { return self.array.get_unchecked(idx.idx); }
    }

    // Get an iterator over the indices of the array
    pub fn indices(&self) -> Indices<'id> {
        return Indices {
            min: 0,
            max: self.array.len(),
            _id: PhantomData
        };
    }
}

impl<'id> Iterator for Indices<'id> {
    type Item = Index<'id>;
    pub fn next(&mut self) -> Option<Self::Item> {
        if self.min == self.max {
            return None;
        } else {
            self.min += 1;
            return Some(Index {
                idx: self.min - 1,
                _id: PhantomData
            });
        }
    }
}
\end{minted}

That's a \emph{lot} of work to safely avoid bounds checks, and although there's
only a single line marked as `unsafe`, its soundness relies on a pretty deep
understanding of Rust. Any errors in the interface design could easily make
the whole thing unsound. So we \emph{really} don't recommend doing this. Also, whenever
this interface catches an error, it produces nonsensical lifetime errors because
Rust has no idea what we're doing. That said, it does demonstrate our ability to
model particularly complex constraints.

Those familiar with generativity and type systems may see that we are ultimately
just applying an age-old trick: \emph{universal} types and functions can be combined to
construct \emph{existential} types. In this case
\mintinline{rust}{where F: for<'id> FnOnce(Array<'arr, 'id>)}
is declaring that the function F is universal over all lifetimes that can be chosen
for $id$. The body of any function that satisfies this signature must work with any
$id$ it receives opaquely. In effect, it knows that there \emph{exists} a lifetime,
and nothing else.


